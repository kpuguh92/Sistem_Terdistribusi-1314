<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd"> 
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en"> 
  <head> 
    <meta http-equiv="content-type" content="text/html; charset=utf-8" /> 
    <meta name="description" content="Python module for distributing computations across multiple processors on a single machine, among many machines in a cluster or grid. The computations can be standalone programs or python functions." /> 
    <meta name="keywords" content="dispy, python, parallel processing, distributed computing, cluster computing" /> 
    <title> 
      dispy - Distributed and Parallel Computing with Python
    </title> 

<link rel="stylesheet" type="text/css" href="style.css" />
 
  </head> 
  <body> 
    <div id="page"> 
      <center><div class="title">dispy : Python Framework for Distributed and Parallel Computing</div></center>
      <div id="menu">
	<ul>
          <li><a href="http://sourceforge.net/projects/dispy/files">Download</a></li> 
          <li><a href="http://sourceforge.net/projects/dispy/">Project Details</a></li> 
          <li><a href="http://sourceforge.net/p/dispy/discussion">Forums</a></li>
	</ul>
	<hr />
	<ul>
          <li><a href="index.html">Project Page</a></li> 
          <li><a href="examples.html">Examples</a></li>
          <li><a href="dispynode.html">dispynode</a></li> 
          <li><a href="dispyscheduler.html">dispyscheduler</a></li> 
          <li><a href="dispynetrelay.html">dispynetrelay</a></li>
	</ul>
      </div> 

    <div id="content"> 
<center><h2>dispy</h2></center>

<p>A dispy client program should first create a cluster for a
  computation, which can be either a Python function or a standalone
  program. The cluster can also specify which nodes can execute
  it. Once the cluster is created, the computation can be evaluated as
  many times as necessary, each time with different data, by
  invoking <em>submit</em> method on the cluster. Each
  time <em>submit</em> method is called, it returns <em>DispyJob</em>
  or job, which can later be examined for result of execution, output
  or error messages etc.
</p>

<p>
  While dispy and other components have various options that cover
  rather comprehensive use cases, making it seem complex, most of the
  options have default values that likely work for common cases. For
  example, starting 'dispynode.py' program on each of the nodes on a
  local network and using JobCluster with <tt>computation</tt>, and
  possibly <tt>depends</tt> parameters may be sufficient.
</p>

<p>
  There are two ways to create clusters with
  dispy: <tt>JobCluster</tt> and <tt>SharedJobCluster</tt>. If only
  one instance of dispy may be running at anytime,
  <tt>JobCluster</tt> is simple to use; it already contains a
  scheduler that will schedule jobs to nodes running 'dispynode'. If,
  however, multiple programs using dispy may be running simultaneously,
  <tt>JobCluster</tt> cannot be used - each of the schedulers in each
  instance of dispy will assume the nodes are controlled exclusively
  by each, causing conflicts. Instead, <tt>SharedJobCluster</tt> must
  be used. In this
  case, <a href="dispyscheduler.html">dispyscheduler</a> must also be
  running on some computer and <tt>SharedJobCluster</tt> must
  set <tt>scheduler_node</tt> parameter with the node running
  dispyscheduler (default is the host that
  calls <tt>SharedJobCluster</tt>).
</p>

<h4>JobCluster</h4>

<code>JobCluster(computation, nodes=['*'], depends=[], callback=None, ip_addr=None, ext_ip_addr=None,
           port=51347, node_port=51348, fault_recover=False, dest_path=None, loglevel=logging.WARNING,
           cleanup=True, pulse_interval=None, ping_interval=None, reentrant=False,
           secret='', keyfile=None, certfile=None)
</code>
where
<ul>
<li>
  <tt>computation</tt> is either a Python function or a string. If it
  is a string, it must be path to executable program. This computation
  is sent to nodes in the given cluster. When a job is submitted (to
  invoke computation with arguments), dispynode executes the
  computation with those arguments in isolation - the computation
  should not depend on global state, such as modules imported in the
  main program or global variables etc.
</li>
<li>
  <tt>nodes</tt> is list. Each element must be either a string or a
  pair (tuple of two elements). If element is a string, it must be
  either IP address or host name. If element is a pair, first element
  of pair must be IP address or name and second element must be port
  number where that node is serving (needed if it is different from
  default 51348). This list serves two purposes: dispy initially sends
  a request to all the nodes listed to find out information about them
  (e.g., number of processing units available for dispy), then sends
  given computation to only those nodes that match the listed nodes
  (dispy may know about nodes not listed in this computation, as it
  also broadcasts identification request).  Wildcard '*' can be used
  to match (part of) any IP address; e.g., '192.168.3.*' matches any
  node whose IP address starts with '192.168.3'.  If there are any
  nodes beyond local network, then all such nodes should be mentioned
  in <tt>nodes</tt>. If there are many such nodes (on outside local
  network), it may be cumbersome to list them all (it is not possible
  to send identification request to outside networks with wildcard in
  them); in that case, <a href="dispynetrelay.html">dispynetrelay</a>
  may be started on one of the nodes on that network and the node
  running dispynetrelay should be added to nodes list (and a wildcard
  for that network, so that other nodes on that network match that
  wildcard); see below for examples.
</li>
<li>
  <tt>depends</tt> is list of dependencies needed
  for <tt>computation</tt>. Each element of this list can be either
  Python function or Python class or an instance of class (object) or
  a Python module or path to file. Only Python modules that are not
  present on nodes already need be listed; standard modules that are
  present on all nodes do not need to be listed here.
</li>
<li>
  <tt>callback</tt> is a function. When a job's results become
  available, dispy will call provided callback function with that job
  as the argument. If a job sends provisional results with
  'dispy_provisional_result' multiple times, then dispy will call
  provided callback each such time. The (provisional) results of
  computation can be retrieved with 'result' field of job, etc. While
  computations are run on nodes in isolated environments, callbacks
  are run in the context of user programs from which
  (Shared)JobCluster is called - for example, callbacks can access
  global variables in programs that created cluster(s).
</li>
<li>
  <tt>ip_addr</tt> is IP address to use for (client) communication. If
  it is not set, all configured network interfaces are used. If it is
  a string, it must be either a host name or IP address, in which case
  corresponding interface address is used. If it is a list, each must
  be a string of host name or IP address, in which case interface
  addresses for each of those is used.
</li>
<li>
  <tt>ext_ip_addr</tt> is a string or list of strings of host names or
  IP addresses to use as return address for node to client
  communication. This may be needed in case the client is behind a NAT
  firewall/gateway and (some) nodes are outside. Typically, in such a
  case, ext_ip_addr must be the address of NAT firewall/gateway and
  the NAT firewall/gateway must forward ports to ip_addr
  appropriately. See below for more on NAT/firewall information.
</li>
<li>
  <tt>port</tt> is port to use for (client) communication. Usually not
  necessary. If not given, dispy will request socket library to choose
  any available port.
</li>
<li>
  <tt>node_port</tt> is port to use for communicating with nodes
  (servers). If this is different from default, 'dispynode' programs
  must be run with the same port.
</li>
<li>
  <tt>dest_path</tt> is directory on the nodes where files are
  transferred to. Default is to create a separate directory for each
  computation. If a computation transfers files (dependencies) and
  same computation is run again with same files, the transfer can be
  avoided by specifying same dest_path, along with the option
  'cleanup=False'.
</li>
<li>
  <tt>fault_recover</tt> must be either True or a string. If it is a
  string, dispy uses it as path to file where it stores information
  about running jobs (jobs scheduled but not finished yet). In case
  user program terminates unexpectedly (for example, because of
  network failure, uncaught exception), the results of submitted jobs
  can be later retrieved through 'fault_recover_jobs' function (see
  below). If this option is True, dispy will store information about
  jobs in a file of the form '_dispy_fault_recover_YYYYMMDDHHMMSS' in
  the current directory. Note that dispy keeps information about only
  the jobs that have been submitted for execution but not finished
  yet. Once a job is finished (i.e., job result is received by dispy's
  scheduler), dispy deletes its information (for time and space
  efficiency). If it is necessary to keep the information about
  finished jobs, callbacks can be used to persist job results.
</li>
<li>
  <tt>loglevel</tt> is message priority
  for <a href="http://docs.python.org/2/library/logging.html">logging</a>
  module.
</li>
<li>
  <tt>cleanup</tt>: Whether any files transferred should be deleted
  after the computation is done. If it is False, the files are left on
  the nodes; this may speedup if same files are needed for another
  cluster later. However, this can be security risk and/or require
  manual cleanup. If same files are used for multiple clusters, then
  cleanup may be set to False and same <tt>dest_path</tt> used.
</li>
<li>
  <tt>pulse_interval</tt> is number of seconds between 'pulse'
  messages that nodes send to indicate they are alive and computing
  submitted jobs. If this value is given as an integer or floating
  number between 1 and 600, then a node is presumed dead if
  5*pulse_interval seconds elapse without a pulse message. See
  'reentrant' below.
</li>
<li>
  <tt>reentrant</tt> must be either True or False. This value is used
  only if 'pulse_interval' is set for any of the clusters. If
  pulse_interval is given and reentrant is False (default), jobs
  scheduled for a dead node are automatically cancelled (for such jobs,
  execution result, output and error fields are set to None, exception
  field is set to 'Cancelled' and status is set to Cancelled); if
  reentrant is True, then jobs scheduled for a dead node are resubmitted
  to other available nodes.
</li>
<li>
  <tt>ping_interval</tt> is number of seconds.  Normally dispy can
  locate nodes running dispynode by broadcasting UDP ping messages on
  local network and point-to-point UDP messages to nodes on remote
  networks. However, UDP messages may get lost.  Ping interval is
  number of seconds between repeated ping messages to find any nodes
  that have missed previous ping messages.
  </li>
<li>
  <tt>secret</tt> is a string that is (hashed and) used for
  handshaking of communication with nodes. This prevents unauthorized
  use of nodes. However, the hashed string (not the secret itself) is
  passed in clear text, so an unauthorized, determined person may be
  able to figure out how to circumvent.
</li>
<li>
  <tt>keyfile</tt> is path to file containing private key for SSL
  communication (see Python 'ssl' module). This key may be stored in
  'certfile' itself, in which case this should be None.
</li>
<li>
  <tt>certfile</tt> is path to file containing SSL certificate (see Python
  'ssl' module).
</li>
</ul>

<h4>SharedJobCluster</h4>
<p>
  SharedJobCluster has almost the same syntax, except as noted below.
  </p>
<code>SharedJobCluster(computation, nodes=['*'], depends=[], ip_addr=None, port=None,
          scheduler_node=None, scheduler_port=None, ext_ip_addr=None
          dest_path=None, loglevel=logging.WARNING, cleanup=True, reentrant=False,
          secret='', keyfile=None, certfile=None)
</code>
where all arguments common to <tt>JobCluster</tt> are same, and
<ul>
<li>
  <tt>port</tt> is client's port (which is 51347 in the case of
  JobCluster) is, by default, choosen randomly to an unused port. This
  is so that in case dispyscheduler is running on the same computer,
  the client port would be different from the port used by
  dispyscheduler. In case the client needs to get results from
  dispyscheduler in remote network and port forwarding is used, a
  specific port (e.g., 51347) can be given.
<li>
<li>
  <tt>scheduler_node</tt> is either IP address or host name
  where <a href="dispyscheduler.html">dispyscheduler</a> is running; if
  it is not given, the node where SharedJobCluster is invoked is
  used
</li>
<li>
  <tt>pulse_interval</tt> is not used in case of SharedJobCluster;
  instead, 'dispyscheduler' must be called with '--pulse_interval'
  option appropriately.
</li>
<li>
  <tt>secret</tt> is a string that is (hashed and) used for
  handshaking of communication with dispyscheduler.
</li>
</ul>

A cluster has following methods:
<ul>
<li>
  <tt>cluster.submit</tt> method is available for the cluster returned
  from JobCluster. This method should be called with the arguments
  exactly as expected by the 'computation' given to JobCluster. If
  'computation' is a Python function, the arguments may also contain
  keyword arguments. However, all arguments must be serializable
  (picklable). If an argument is a class object that contains
  non-serializable members, then the classes may
  provide <tt>__getstate__</tt> method for this purpose (see '_Job'
  class in dispy.py for an example). If 'computation' is a program, then
  all arguments must be strings.

  <tt>submit</tt> returns a <tt>DispyJob</tt> object (see below for
  useful fields of this object). Results from execution of computation
  with given arguments will be available in the job object after
  execution finishes.
</li>

<li>
  <tt>cluster()</tt> will wait for all submitted jobs to complete.
</li>

<li>
  <tt>cluster.wait()</tt> will wait for all submitted jobs to complete.
</li>

<li>
  <tt>cluster.close()</tt> will wait for all submitted jobs to complete
  and then cleanup (such as removing any transferred files, deleting
  'computation' from the nodes etc.).
</li>

<li>
  <tt>cluster.cancel(job)</tt> will remove the job submitted, by
  terminating it if it already started execution. Note that if the job
  execution has any side effects (such as updating database, files
  etc.), cancelling a job may leave unpredictable side effects,
  depending on at what point job is cancelled.
</li>

<li>
  <tt>cluster.stats()</tt> will print statistics about nodes, time
  each node spent executing jobs etc.
</li>
</ul>

<h4>DispyJob</h4>

<p>
 The result of <tt>submit</tt> call of a cluster is an instance of
DispyJob (see dispy.py), which can be used to examine status of job
execution, retrieve job results etc. The job instance has <tt>id</tt>
field that can be used to set any value appropriate (rest of the
fields are either read-only, or not meant for user programs). For
example, <tt>id</tt> field can be set to a unique value to distinguish
one job from another.
</p>

<p>
Job's <tt>status</tt> field is read-only field; its value is one of
<tt>Created</tt>, <tt>Running</tt>, <tt>Finished</tt>, <tt>Cancelled</tt>
or <tt>Terminated</tt>, indicating current status of job.  If job is
created for SharedJobCluster, <tt>status</tt> is not updated to
<tt>Running</tt> when job is actually running.
</p>
  
<p>
When a submitted job is called with job(), it returns that job's
execution result, possibly waiting until the job is finished. After a
job is complete,
</p>
<ul>
  <li>job.result will have computation's result - return value if
  computation is a function and exit status if it is a
  program. job.result is same as return value of job() </li>
  <li>job.stdout and job.stderr will have stdout and stderr strings</li>
  <li>job.exception will have exception trace if computation raises
    any exception; in this case the result of job.result will
    be <tt>None</tt></li>
  <li>job.start_time will be the time when job is scheduled for execution on a node</li>
  <li>job.end_time will be time when results became available</li>
  </ul>

Job's result, stdout and stderr should not be large - these are
buffered and hence will consume memory (not stored on disk). Moreover,
like args and kwargs, result should be serializable (picklable
object). If result is (or has) an instance of python class, that class
may have to provide __getstate__ function to serialize the object.

<p>
After jobs are submitted, cluster.wait() can be used to wait until all
submitted jobs for that cluster have finished. If necessary, results
of execution can be retrieved by either job() or job.result, as
described above.
</p>


<h4>Fault Recovery</h4>
<p>
As noted above, if 'fault_recover' option is used when creating a
cluster, dispy stores information about scheduled but unfinished jobs
in a file. If user program then terminates unexpectedly, the nodes
that execute those jobs can't send the results back to dispy. In such
cases, the results for the jobs can be retrieved from the nodes with
the function in dispy</p>
<code>fault_recover_jobs(fault_recover_file, ip_addr=None, secret='', node_port=51348,
                    certfile=None, keyfile=None)</code>
where
<ul>
  <li><tt>fault_recover_file</tt> is path to the file used or created
  when JobCluster is used.</li>
  <li><tt>ip_addr</tt> is IP address to use for (client)
    communication. This may be needed in case the client has multiple
    interfaces and default interface is not the right choice (this
    would be same as the 'ip_addr' option used for JobCluster).</li>
  <li><tt>secret</tt> is a string that is (hashed and) used for
  handshaking of communication with nodes (should same as the one used
  when creating JobCluster).</li>
  <li><tt>node_port</tt> is port to use for communicating with nodes
  (servers). If this should be different from default, 'dispynode'
  programs must be run with the same port. This option should be same
  as the one used when creating JobCluster.</li>
  <li><tt>certfile</tt> is path to file containing SSL certificate (see Python
  'ssl' module) (same as the one used when creating JobCluster).</li>
  <li><tt>keyfile</tt> is path to file containing private key for SSL
  communication (see Python 'ssl' module). This key may be stored in
  'certfile' itself, in which case this should be None. This option
  should be same as the one used when creating JobCluster.</li>
</ul>

<p>
This function reads the information about jobs in the
fault_recover_file, retrieves DispyJob instance (that contains
results, stdout, stderr, status etc.) for each job that was scheduled
for execution but unfinished at the time of crash, and returns them as
a list. If a job has finished executing at the time
'fault_recover_jobs' function is called, the information about that is
deleted from both the node and fault_recover_file, so the results for
finished jobs can't be retrieved more than once. However, if a job is
still executing, the status field of DispyJob would be
DispyJob.Running and the results for this job can be retrieved again
(until that job finishes) by calling 'fault_recover_jobs'. Note that
'fault_recover_jobs' is available as separate function - it doesn't
need JobCluster or SharedJobCluster instance. In fact,
'fault_recover_jobs' function must not be used when a cluster that
uses same recover file is currently running.
</p>

<p>
Note that dispy sends only the given computation and its dependencies
to the nodes; the program itself is not transferred. So if computation
is a python function, it must import all the modules used by it, even
if the program imported those modules before cluster is created.
</p>

<h4>Provisional/Intermediate Results</h4>
<p>
  <em>dispy_provisional_result</em> function can be used in
  computations (Python functions) to send provisional or intermediate
  results back to the client. For example, in optimization
  computations, there may be many (sub) optimal results that the
  computations can inform the client. The client may ignore, cancel
  computations or create additional computations based on provisional
  result. When computation calls
  <em>dispy_provisional_result(result)</em>, the Python
  object <em>result</em> (which must be serializable) is sent back to
  the client and computation continues to execute. The client should
  use callback option to process the information, as shown in the
  example:
  </p>
<pre><code>import random, dispy

def compute(n, threshold):
    import random, time, socket
    name = socket.gethostname()
    for i in xrange(0, n):
        r = random.uniform(0, 1)
        if r <= threshold:
            # possible result
            dispy_provisional_result((name, r))
        time.sleep(0.1)
    # final result
    return None

def job_callback(job):
    if job.status == dispy.DispyJob.ProvisionalResult:
        if job.result[1] < 0.005:
            # acceptable result; terminate jobs
            print '%s computed: %s' % (job.result[0], job.result[1])
            # global jobs, cluster
            for j in jobs:
                if j.status in [dispy.DispyJob.Created, dispy.DispyJob.Running,
			        dispy.DispyJob.ProvisionalResult]:
                    cluster.cancel(j)

if __name__ == '__main__':
    cluster = dispy.JobCluster(compute, callback=job_callback)
    jobs = []
    for n in xrange(4):
        job = cluster.submit(random.randint(50,100), 0.2)
        if job is None:
            print 'creating job %s failed!' % n
            continue
        job.id = n
        jobs.append(job)
    cluster.wait()
    cluster.stats()
    cluster.close()
  </code></pre>
<p>
  In the above example, computations send provisional result if
  computed number is &lt;= threshold (0.2). If the number computed is &lt;
  0.005, job_callback deems it acceptable and terminates computations.
  </p>

<h4><a name="NAT" />NAT/Firewall Forwarding</h4>
<p>
By default dispy client uses UDP and TCP ports 51347, dispynode uses
UDP and TCP ports 51348, and dispyscheduler uses UDP and TCP pots
51347 and TCP port 51348. If client/node/scheduler are behind a NAT
firewall/gateway, then these ports must be forwarded appropriately and
'ext_ip_addr' option must be used. For example, if dispy client is
behdind NAT firewall/gateway, JobCluster/SharedJobCluster must set
'ext_ip_addr' to the NAT firewall/gateway address and forward UDP and
TCP ports 51347 to the IP address where client is running. Similarly,
if dispynode is behind NAT firewall/gateway, 'ext_ip_addr' option must
be used.
  </p>

<h4>Cloud Computing with Amazon EC2</h4>
<p>
  <tt>ext_ip_addr</tt> option can be used to work with Amazon EC2
  cloud computing service. With EC2 service, a node has a private IP
  address (called 'Private DNS Address') that uses private network of
  the form 10.x.x.x and public address (called 'Public DNS Address')
  that is of the form ec2-x-x-x-x.x.amazonaws.com. After launching
  instance(s), one can copy dispy files to the node(s) and run
  dispynode as
  <tt>dispynode.py --ext_ip_addr ec2-x-x-x-x.y.amazonaws.com</tt>
  (this address can't be used with '-i'/'--ip_addr' option, as the
  network interface is configured with private IP address only). This
  node can then be used by dispy client from outside EC2 network by
  specifying ec2-x-x-x-x.x.amazonaws.com in the 'nodes' list (thus,
  using EC2 servers to augment processing units). Roughly, dispy uses
  'ext_ip_addr' similar to NAT - it announces 'ext_ip_addr' to
  other services instead of the configured 'ip_addr' so that external
  services send requests to 'ext_ip_addr' and if firewall/gateway
  forwards them appropriately, dispy will process them.
  </p>


<p>
dispy can also be used as a command line tool; in this case the
computations should only be programs and dependencies should only be
files.
</p>

  <tt>dispy.py -f /some/file1 -f file2 -a "arg11 arg12" -a "arg21 arg22" -a "arg3" /some/program</tt><br />

  will distribute '/some/program' with dependencies '/some/file1' and
  'file2' and then execute '/some/program' in parallel with arg11 and arg12 (two
  arguments to the program), arg21 and arg22 (two arguments), and arg3
  (one argument).

      </div> 
    </div> 
    <div id="footer"> 
      <p> 
        <a href="http://sourceforge.net/"> 
          Project Web Hosted by <img src="http://sflogo.sourceforge.net/sflogo.php?group_id=539226&amp;type=3" alt="SourceForge.net" /> 
        </a> 
      </p> 
      <p> 
        &copy;Copyright 1999-2009 -
        <a href="http://geek.net" title="Network which provides and promotes Open Source software downloads, development, discussion and news."> 
          Geeknet</a>, Inc., All Rights Reserved
      </p> 
      <p> 
        <a href="http://sourceforge.net/about"> 
          About
        </a> 
        -
        <a href="http://sourceforge.net/tos/tos.php"> 
          Legal
        </a> 
        -
        <a href="http://p.sf.net/sourceforge/getsupport"> 
          Help
        </a> 
      </p> 
    </div> 

  </body> 
</html> 
