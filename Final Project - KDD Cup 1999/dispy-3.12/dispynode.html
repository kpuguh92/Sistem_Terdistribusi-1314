<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd"> 
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en"> 
  <head> 
    <meta http-equiv="content-type" content="text/html; charset=utf-8" /> 
    <meta name="description" content="Python module for distributing computations across multiple processors on a single machine, among many machines in a cluster or grid. The computations can be standalone programs or python functions." /> 
    <meta name="keywords" content="dispy, python, parallel processing, distributed computing, cluster computing" /> 
    <title> 
      dispy - Distributed and Parallel Computing with Python
    </title> 

<link rel="stylesheet" type="text/css" href="style.css" />

  </head> 
  <body> 
    <div id="page"> 
      <center><div class="title">dispy : Python Framework for Distributed and Parallel Computing</div></center>
      <div id="menu">
	<ul>
          <li><a href="http://sourceforge.net/projects/dispy/files">Download</a></li> 
          <li><a href="http://sourceforge.net/projects/dispy/">Project Details</a></li> 
          <li><a href="http://sourceforge.net/p/dispy/discussion">Forums</a></li>
	</ul>
	<hr />
	<ul>
          <li><a href="index.html">Project Page</a></li> 
          <li><a href="examples.html">Examples</a></li>
          <li><a href="dispy.html">dispy</a></li> 
          <li><a href="dispyscheduler.html">dispyscheduler</a></li> 
          <li><a href="dispynetrelay.html">dispynetrelay</a></li>
	</ul>
      </div> 

    <div id="content"> 
      <center><h2>dispynode</h2></center>
<p>
  'dispynode.py' (or dispynode) program should be running on each of
  the nodes. It executes jobs for dispy clients. Usually,
  no options are needed; '-d' option may be useful to see log of jobs
  being executed.
</p>

<p>Below are various options to invoke dispynode:
  </p>
  <ul>
    <li><tt>-d</tt> enables debug messages that show trace of
    execution.</li>
    <li><tt>-c n</tt> or <tt>--cpus=n</tt> sets the number of
    processing units to <tt>n</tt>. Without this option, dispynode
    will use all the processing units available on that
    node. If <tt>n</tt> is positive, it must be at least 1 and at most
    number of processing units on that node; dispynode will then use
    at most <tt>n</tt> processors. If <tt>n</tt> is negative, then
    that many processing units are not used by dispynode.</li>
    <li><tt>-i addr</tt> or <tt>--ip_addr=addr</tt> directs dispynode
    to use given <tt>addr</tt> for communication, instead of the IP
    address associated with the host name.</li>
    <li>
      <tt>--ext_ip_addr=addr</tt> directs dispynode to
      announce <tt>addr</tt> in network communication so that the node
      can be used if it is behind NAT firewall/gateway that is
      configured to use <tt>addr</tt>. See below.
      </li>
    <li><tt>-p n</tt> or <tt>--node_port=n</tt> directs dispynode to
    use given port <tt>n</tt> instead of default port 51348.</li>
    <li><tt>-s secret</tt> or <tt>--secret=secret</tt> directs
    dispynode to use 'secret' for hashing handshake communication with
    dispy scheduler.</li>
    <li><tt>--dest_path_prefix=path</tt> directs dispynode to use
    'path' as prefix for storing files sent by dispy scheduler. If a
    cluster uses <tt>dest_path</tt> option (when creating cluster with
    JobCluster or SharedJobCluster), then <tt>dest_path</tt> is
    appened to 'path' prefix. With this, files from different clusters
    can be automatically stored in different directories, to avoid
    conflicts. Unless <tt>cleanup=False</tt> option is used when
    creating a cluster, dispynode will remove all files and
    directories created after the cluster is terminated.</li>
    <li><tt>--scheduler_node=addr</tt>: If the node is in the same
    network as the dispy scheduler or when no jobs are scheduled at
    the time dispynode is started, this option is not
    necessary. However, if jobs are already scheduled and scheduler
    and node are on different networks, the given <tt>addr</tt> is
    used for handshake with the scheduler.</li>
    <li><tt>--scheduler_port=n</tt> directs dispynode to use
    port <tt>n</tt> to communicate with scheduler. Default value is
    51347. When using this option, make sure dispy scheduler is also
    directed to use same port.</li>
    <li><tt>--keyfile=path</tt> is path to file containing private key
    for SSL communication (see Python 'ssl' module). This key may be
    stored in 'certfile' itself, in which case this can be None.</li>
    <li><tt>--certfile=path</tt> is path to file containing SSL
    certificate (see Python 'ssl' module).</li>
    <li><tt>--max_file_size n</tt> specifies maximum size of any file
    transferred by clusters. If size of a file transferred by cluster
    exceeds <tt>n</tt>, dispynode will silently truncate it.</li>
    <li><tt>--zombie_interval=n</tt> indicates dispynode to assume a
    scheduler is a zombie if there is no communication from it for 'n'
    <strong>minutes</strong>. dispynode doesn't terminate jobs
    submitted by a zombie scheduler; instead, when all the jobs
    scheduled are completed, the node frees itself from that scheduler
    so other schedulers may use the node.</li>
</ul>

<h4>NAT/Firewall Forwarding</h4>

<p>
As explained in <a href="dispy.html">dispy</a> documentation,
'ext_ip_addr' can be used in case dispynode is behding a NAT
firewall/gateway and the NAT forwards UDP and TCP ports 51348 to the
IP address where dispynode is running. Thus, assuming NAT
firewall/gateway is at (public) IP address a.b.c.d, dispynode is to
run at (private) IP address 192.168.5.33 and NAT forwards UDP and TCP
ports 51348 to 192.168.5.33, dispynode can be invoked as
<tt>dispynode.py -i 192.168.5.33 --ext_ip_addr a.b.c.d</tt>. If
multiple dispynodes are needed behind a.b.c.d, then each must be
started with different 'port' argument and those ports must be
forwarded to nodes appropriately. For example, to continue the
example, if 192.168.5.34 is another node that can run dispynode, then
it can be started on it as
<tt>dispynode.py -i 192.168.5.34 -p 51350 --ext_ip_addr a.b.c.d</tt>
and configure NAT to forward UDP and TCP ports 51350 to
192.168.5.34. Then dispy client can use the nodes with
<tt>cluster = JobCluster(compute, nodes=[('a.b.c.d', 51347), ('a.b.c.d', 51350)])</tt>
</p>
      </div> 
    </div> 
    <div id="footer"> 
      <p> 
        <a href="http://sourceforge.net/"> 
          Project Web Hosted by <img src="http://sflogo.sourceforge.net/sflogo.php?group_id=539226&amp;type=3" alt="SourceForge.net" /> 
        </a> 
      </p> 
      <p> 
        &copy;Copyright 1999-2009 -
        <a href="http://geek.net" title="Network which provides and promotes Open Source software downloads, development, discussion and news."> 
          Geeknet</a>, Inc., All Rights Reserved
      </p> 
      <p> 
        <a href="http://sourceforge.net/about"> 
          About
        </a> 
        -
        <a href="http://sourceforge.net/tos/tos.php"> 
          Legal
        </a> 
        -
        <a href="http://p.sf.net/sourceforge/getsupport"> 
          Help
        </a> 
      </p> 
    </div> 

  </body> 
</html> 
